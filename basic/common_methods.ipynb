{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93472cfd",
   "metadata": {},
   "source": [
    "# Common PyTorch Functions and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea08c8f",
   "metadata": {},
   "source": [
    "## Tensor Creation\n",
    "\n",
    "### torch.tensor(): Creates a tensor from data (like a list or array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00738390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d9af14",
   "metadata": {},
   "source": [
    "### torch.zeros(): Creates a tensor filled with 0s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f4884f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2, 3) # 2 rows, 3 columns of 0.0\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da8d47",
   "metadata": {},
   "source": [
    "### torch.ones(): Creates a tensor filled with 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ededff8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2,3,4) # 1D tensor with five 1.0s\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4b0f17",
   "metadata": {},
   "source": [
    "### torch.rand(): Random values from a uniform distribution $[0, 1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3265c725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6515, 0.4992, 0.3089, 0.5694],\n",
       "        [0.9685, 0.2066, 0.6741, 0.1608],\n",
       "        [0.8231, 0.6310, 0.9926, 0.0780]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(3, 4) # Useful for initializing weights randomly\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2617112",
   "metadata": {},
   "source": [
    "### torch.randn(): Random values from a normal distribution (mean 0, var 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7863bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.9755, -2.8533, -0.0843],\n",
       "        [ 1.2666, -0.2682, -0.5117],\n",
       "        [-0.4182,  1.0114, -1.2105]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 3) # Often preferred for initializing neural network layers\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528b3fc",
   "metadata": {},
   "source": [
    "### torch.arange(): Returns a 1D tensor with a range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b673081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(start=0, end=10, step=2) # tensor([0, 2, 4, 6, 8])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d65b7",
   "metadata": {},
   "source": [
    "### torch.linspace(): Creates evenly spaced values between a start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57312829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.linspace(start=0, end=1, steps=5) # tensor([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918e9417",
   "metadata": {},
   "source": [
    "### torch.eye(): Creates an identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd77d762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3) # 3x3 Identity matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11cbe5d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(4, 2) # 4x2 Identity matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "195e9a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(5, 3) # 5x3 Identity matrix\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ba9ca",
   "metadata": {},
   "source": [
    "### torch.full(): Creates a tensor filled with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6da172c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416],\n",
       "        [3.1416, 3.1416]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.full((2, 2), 3.14159) # A 2x2 tensor of Pi\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e086fe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416],\n",
       "        [3.1416, 3.1416, 3.1416]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.full((4, 3), 3.14159) # A 4x3 tensor of Pi\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fdc17d",
   "metadata": {},
   "source": [
    "### torch.from_numpy(): Converts a NumPy array into a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac0d81ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.array([1, 2, 3])\n",
    "t = torch.from_numpy(n)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e877d62",
   "metadata": {},
   "source": [
    "### torch.zeros_like(): Creates a tensor of 0s with the same shape as another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f60e2aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros_like(x) # x is any existing tensor\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4ebd8",
   "metadata": {},
   "source": [
    "### torch.ones_like(): Creates a tensor of 1s with the same shape as another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38bcdca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones_like(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023c46a",
   "metadata": {},
   "source": [
    "### dtype: Concept of data types (e.g., torch.float32, torch.int64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb2b905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], dtype=torch.float16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, dtype=torch.float16) # Saves memory compared to float32\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c1098",
   "metadata": {},
   "source": [
    "### device: Concept of where the tensor lives (cpu, cuda, or mps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "983a8d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9810, 0.5945, 0.9302],\n",
       "        [0.4438, 0.0609, 0.8142],\n",
       "        [0.7583, 0.6873, 0.9753]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Moving a tensor to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x = torch.rand(3, 3).to(device)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ffbec",
   "metadata": {},
   "source": [
    "### requires_grad: A flag that enables gradient tracking for a tensor.\n",
    "\n",
    "- If set to True, PyTorch starts tracking every operation on this tensor for automatic differentiation (calculating gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "088f96f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6116,  0.8640,  0.6185, -0.6311,  0.3483],\n",
       "        [ 1.5945,  0.4967,  0.2957, -1.7618,  0.9806],\n",
       "        [ 0.3773,  0.7827, -0.9259,  0.1817,  0.5624],\n",
       "        [-0.9019,  0.5726,  0.3483, -1.5594, -0.7715],\n",
       "        [ 1.6089,  1.4334, -0.0570,  0.2737, -0.0631],\n",
       "        [-0.1730, -1.1494,  0.1716,  0.6753,  0.7992],\n",
       "        [ 0.8921, -0.8448,  0.9771,  0.8726,  1.2721],\n",
       "        [ 0.1113, -1.3985, -0.1056, -1.3583, -1.9216],\n",
       "        [-0.7093, -0.1859,  1.5060, -0.5990,  1.0691],\n",
       "        [-0.5215, -1.1779,  0.5231,  0.0641, -0.7135]], requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights in a model need this to be True to learn!\n",
    "weights = torch.randn(10, 5, requires_grad=True) # Default is False\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7ec13",
   "metadata": {},
   "source": [
    "## Tensor Shaping & Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c61d6fa",
   "metadata": {},
   "source": [
    "### view(): \n",
    "- Returns a new tensor with the same data but a different shape. The new shape must have the same number of elements as the original.\n",
    "\n",
    "- Constraint: It only works on \"contiguous\" tensors (data stored in a single block of memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd086135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([4, 4])\n",
      "Reshaped to 2x8: torch.Size([2, 8])\n",
      "Reshaped to 8x2: torch.Size([8, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4) # 16 elements\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.view(2, 8)      # Reshaped to 2x8\n",
    "print(\"Reshaped to 2x8:\", y.shape)\n",
    "z = x.view(-1, 2)     # -1 tells PyTorch to \"figure out\" the dimension (results in 8x2)\n",
    "print(\"Reshaped to 8x2:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a97a8e",
   "metadata": {},
   "source": [
    "### **reshape():** \n",
    "- Similar to view, but more robust. If the tensor isn't contiguous, it will copy the data to a new memory block automatically.\n",
    "\n",
    "### **flatten():** \n",
    "- Collapses a range of dimensions into one. Often used to turn a 2D image into a 1D vector before a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79106c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 4])\n",
      "Reshaped to 2x6: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.reshape(2, 6)  # Reshaped to 2x6\n",
    "print(\"Reshaped to 2x6:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0785d992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 28, 28])\n",
      "Flattened shape: torch.Size([784])\n",
      "Reshaped to 1D shape: torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 28, 28)\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.flatten() # Shape: [784]\n",
    "print(\"Flattened shape:\", y.shape)\n",
    "z = x.view(-1)  # Shape: [784]\n",
    "print(\"Reshaped to 1D shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d5d8d5",
   "metadata": {},
   "source": [
    "### **transpose():**\n",
    "- Swaps exactly two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8554584d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3671,  1.2576, -0.6083],\n",
       "        [ 1.2173, -0.3585,  0.1795],\n",
       "        [ 1.2256,  0.4310,  0.6151],\n",
       "        [-0.9671,  1.2287, -1.2524],\n",
       "        [ 0.1616, -1.1140,  0.2838]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 5)\n",
    "y = x.transpose(0, 1) # Shape: [5, 3]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4074dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape:\n",
      " tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5,  9],\n",
       "        [ 2,  6, 10],\n",
       "        [ 3,  7, 11],\n",
       "        [ 4,  8, 12]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])  # 3x4 tensor\n",
    "print(\"Original shape:\\n\", x)\n",
    "y = x.t()  # Transposed to 4x3\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4d0997",
   "metadata": {},
   "source": [
    "### **permute():** \n",
    "- A more powerful version of transpose that can reorder any number of dimensions at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5450c611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([16, 3, 64, 64])\n",
      "Permuted shape: torch.Size([16, 64, 64, 3])\n"
     ]
    }
   ],
   "source": [
    "# Useful for converting (Batch, Channels, H, W) -> (Batch, H, W, Channels)\n",
    "x = torch.randn(16, 3, 64, 64)\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.permute(0, 2, 3, 1) # Shape: [16, 64, 64, 3]\n",
    "print(\"Permuted shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afbdac3",
   "metadata": {},
   "source": [
    "### **squeeze():** \n",
    "- Removes all dimensions of size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6798fddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 5, 1, 3])\n",
      "Squeezed shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 5, 1, 3)\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.squeeze() # Shape: [5, 3]\n",
    "print(\"Squeezed shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d12f1ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 2, 3, 4])\n",
      "Squeezed shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(1, 2, 3, 4)\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.squeeze() # Shape: [2, 3, 4], Removes only dimension 1 (second dimension) \n",
    "print(\"Squeezed shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff62ef4",
   "metadata": {},
   "source": [
    "### **unsqueeze():** \n",
    "- Adds a dimension of size 1 at the specified index. This is essential for adding a \"batch\" dimension to a single input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2189ed1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 224, 224])\n",
      "Unsqueezed shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 224, 224) # A single RGB image\n",
    "print(\"Original shape:\", x.shape)\n",
    "y = x.unsqueeze(0)           # Shape: [1, 3, 224, 224] (Ready for a model)\n",
    "print(\"Unsqueezed shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4806fe",
   "metadata": {},
   "source": [
    "### **torch.cat():** \n",
    "- Concatenates tensors along an existing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "317117c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 3)\n",
    "c = torch.cat((a, b), dim=0) # Shape: [4, 3]\n",
    "print(\"Concatenated shape:\", c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d6ed18",
   "metadata": {},
   "source": [
    "### **torch.stack():** \n",
    "- Joins tensors along a new dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a4187d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "c = torch.stack((a, b), dim=0) # Shape: [2, 2, 3]\n",
    "print(\"Stacked shape:\", c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f7e8d",
   "metadata": {},
   "source": [
    "### **torch.split():** \n",
    "- Breaks a tensor into chunks of a specific size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a8dea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split tensors (size=2 along dim=1): (tensor([[-1.2095,  0.0408],\n",
      "        [-0.6972,  1.3195],\n",
      "        [ 0.7223, -0.2128],\n",
      "        [ 1.5339,  0.1758]]), tensor([[-1.0989, -0.3553],\n",
      "        [-0.7979,  0.3025],\n",
      "        [-0.0906,  0.8457],\n",
      "        [-0.3977,  0.3002]]))\n",
      "Split tensors (sizes [1,3]): (tensor([[-1.2095],\n",
      "        [-0.6972],\n",
      "        [ 0.7223],\n",
      "        [ 1.5339]]), tensor([[ 0.0408, -1.0989, -0.3553],\n",
      "        [ 1.3195, -0.7979,  0.3025],\n",
      "        [-0.2128, -0.0906,  0.8457],\n",
      "        [ 0.1758, -0.3977,  0.3002]]))\n"
     ]
    }
   ],
   "source": [
    "# Example: split tensor into chunks of size 2 along dim=1\n",
    "x = torch.randn(4, 4)\n",
    "c = torch.split(x, 2, dim=1)  # returns a tuple of tensors\n",
    "print(\"Split tensors (size=2 along dim=1):\", c)\n",
    "\n",
    "# Example: split with a list of sizes\n",
    "d = torch.split(x, [1, 3], dim=1)\n",
    "print(\"Split tensors (sizes [1,3]):\", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0861c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([4, 6])\n",
      "Number of chunks: 2\n",
      "Shape of each chunk: [torch.Size([2, 6]), torch.Size([2, 6])]\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk(): Splits a tensor into a specific number of chunks along a dimension\n",
    "x = torch.randn(4, 6)\n",
    "print(\"Original shape:\", x.shape)\n",
    "chunks = torch.chunk(x, chunks=2, dim=0)  # Split into 2 chunks along dimension 0\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"Shape of each chunk:\", [chunk.shape for chunk in chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ecaebf",
   "metadata": {},
   "source": [
    "### **gather():** \n",
    "- Collects values from a tensor along an axis using an index tensor. (Common in Reinforcement Learning to select specific action probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ad6e85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[-1.8226, -1.2664, -1.1982],\n",
      "        [-0.6893,  1.5571, -0.2017]])\n",
      "idx1:\n",
      " tensor([[0, 2, 1],\n",
      "        [1, 0, 2]])\n",
      "gather(a, dim=1, idx1):\n",
      " tensor([[-1.8226, -1.1982, -1.2664],\n",
      "        [ 1.5571, -0.6893, -0.2017]])\n",
      "\n",
      "b:\n",
      " tensor([[-1.4220,  0.7331, -1.4626],\n",
      "        [ 0.2823, -1.3231, -1.4902]])\n",
      "idx2:\n",
      " tensor([[1, 0, 1],\n",
      "        [0, 1, 0]])\n",
      "gather(b, dim=0, idx2):\n",
      " tensor([[ 0.2823,  0.7331, -1.4902],\n",
      "        [-1.4220, -1.3231, -1.4626]])\n",
      "\n",
      "x:\n",
      " tensor([[ 0.5097, -0.5562,  0.1617, -0.4310, -0.4496, -0.2203],\n",
      "        [-0.0255,  0.5188, -0.6625, -0.6019,  0.1647,  0.8892],\n",
      "        [ 1.1193, -0.9448,  0.5424, -0.4241, -0.0626, -1.5036],\n",
      "        [ 0.2396, -0.5232, -0.5933, -0.1551, -3.1055, -0.5005]])\n",
      "idx3 (one index per row):\n",
      " tensor([[3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [4]])\n",
      "gather(x, dim=1, idx3):\n",
      " tensor([[-0.4310],\n",
      "        [-0.6625],\n",
      "        [ 1.1193],\n",
      "        [-3.1055]])\n",
      "\n",
      "Selected action values from weights:\n",
      " tensor([[ 0.6116],\n",
      "        [ 0.4967],\n",
      "        [-0.9259],\n",
      "        [-1.5594],\n",
      "        [-0.0631],\n",
      "        [-0.1730],\n",
      "        [-0.8448],\n",
      "        [-0.1056],\n",
      "        [-0.5990],\n",
      "        [-0.7135]], grad_fn=<GatherBackward0>)\n",
      "\n",
      "y.shape: torch.Size([1, 2, 3, 4])\n",
      "idx_y.shape: torch.Size([1, 2, 3, 1])\n",
      "gather(y, dim=3, idx_y).shape: torch.Size([1, 2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "# Examples of torch.gather using tensors already defined in the notebook.\n",
    "\n",
    "# 1) 2D gather along dim=1 (select columns per row) using 'a' (2x3)\n",
    "idx1 = torch.tensor([[0, 2, 1],\n",
    "                     [1, 0, 2]], dtype=torch.long)\n",
    "res1 = torch.gather(a, dim=1, index=idx1)\n",
    "print(\"a:\\n\", a)\n",
    "print(\"idx1:\\n\", idx1)\n",
    "print(\"gather(a, dim=1, idx1):\\n\", res1)\n",
    "\n",
    "# 2) gather along dim=0 (select rows per column) using 'b' (2x3)\n",
    "idx2 = torch.tensor([[1, 0, 1],\n",
    "                     [0, 1, 0]], dtype=torch.long)\n",
    "res2 = torch.gather(b, dim=0, index=idx2)\n",
    "print(\"\\nb:\\n\", b)\n",
    "print(\"idx2:\\n\", idx2)\n",
    "print(\"gather(b, dim=0, idx2):\\n\", res2)\n",
    "\n",
    "# 3) Select one column per row from x (4x6)\n",
    "idx3 = torch.randint(0, x.size(1), (x.size(0), 1), dtype=torch.long)  # shape [4,1]\n",
    "res3 = torch.gather(x, dim=1, index=idx3)  # shape [4,1]\n",
    "print(\"\\nx:\\n\", x)\n",
    "print(\"idx3 (one index per row):\\n\", idx3)\n",
    "print(\"gather(x, dim=1, idx3):\\n\", res3)\n",
    "\n",
    "# 4) RL-style: select action-values from 'weights' (10x5) given action indices\n",
    "actions = torch.tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=torch.long).unsqueeze(1)  # [10,1]\n",
    "selected_values = torch.gather(weights, dim=1, index=actions)  # [10,1]\n",
    "print(\"\\nSelected action values from weights:\\n\", selected_values)\n",
    "\n",
    "# 5) 4D gather: use 'y' (1x2x3x4) and pick one element from last dim per position\n",
    "idx_y = torch.randint(0, y.size(-1), (y.size(0), y.size(1), y.size(2), 1), dtype=torch.long)  # [...,1]\n",
    "res_y = torch.gather(y, dim=3, index=idx_y)\n",
    "print(\"\\ny.shape:\", y.shape)\n",
    "print(\"idx_y.shape:\", idx_y.shape)\n",
    "print(\"gather(y, dim=3, idx_y).shape:\", res_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d3f4f",
   "metadata": {},
   "source": [
    "### **scatter_():** \n",
    "- The opposite of gather; it writes values into a tensor at specified indices. (The _ means it happens in-place)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "776f99cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scatter_ basic ->\n",
      " tensor([[10., 30., 20.],\n",
      "        [50., 40., 60.]])\n",
      "scatter_add_ ->\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "one_hot shape: torch.Size([10, 5])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "target_y.sum() (number of scattered ones): 6.0\n"
     ]
    }
   ],
   "source": [
    "# 1) Basic scatter_: place src values into target according to idx1 (shape [2,3])\n",
    "target = torch.zeros_like(a)\n",
    "src = torch.tensor([[10., 20., 30.],\n",
    "                    [40., 50., 60.]], dtype=target.dtype)\n",
    "target.scatter_(dim=1, index=idx1, src=src)\n",
    "print(\"scatter_ basic ->\\n\", target)\n",
    "\n",
    "# 2) scatter_add_: accumulate values at indices (useful when indices repeat)\n",
    "t2 = torch.zeros(2, 3, dtype=a.dtype)\n",
    "src2 = torch.tensor([[1., 1., 1.],\n",
    "                     [1., 1., 1.]], dtype=t2.dtype)\n",
    "t2.scatter_add_(dim=1, index=idx1, src=src2)\n",
    "print(\"scatter_add_ ->\\n\", t2)\n",
    "\n",
    "# 3) One-hot encoding from actions (common RL pattern)\n",
    "one_hot = torch.zeros(actions.size(0), weights.size(1), dtype=weights.dtype, device=weights.device)\n",
    "one_hot.scatter_(1, actions, 1.0)\n",
    "print(\"one_hot shape:\", one_hot.shape)\n",
    "print(one_hot[:5])\n",
    "\n",
    "# 4) Scatter into 4D tensor using idx_y (inverse-style of the gather example)\n",
    "target_y = torch.zeros_like(y)\n",
    "src_y = torch.ones_like(idx_y, dtype=y.dtype)\n",
    "target_y.scatter_(3, idx_y, src_y)  # write 1.0 at positions specified in idx_y along last dim\n",
    "print(\"target_y.sum() (number of scattered ones):\", target_y.sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c8516",
   "metadata": {},
   "source": [
    "### **expand():** \n",
    "- Efficiently makes a tensor look larger by \"repeating\" it without actually copying the data in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de07a6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1], [2]]) # Shape: [2, 1]\n",
    "y = x.expand(2, 4)           # Shape: [2, 4], but uses no extra memory!\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f68aad",
   "metadata": {},
   "source": [
    "### **repeat():** \n",
    "- Unlike expand, this physically copies the data to fill the new shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1d412002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5097, -0.5562,  0.1617, -0.4310, -0.4496, -0.2203,  0.5097, -0.5562,\n",
       "          0.1617, -0.4310, -0.4496, -0.2203,  0.5097, -0.5562,  0.1617, -0.4310,\n",
       "         -0.4496, -0.2203,  0.5097, -0.5562,  0.1617, -0.4310, -0.4496, -0.2203],\n",
       "        [-0.0255,  0.5188, -0.6625, -0.6019,  0.1647,  0.8892, -0.0255,  0.5188,\n",
       "         -0.6625, -0.6019,  0.1647,  0.8892, -0.0255,  0.5188, -0.6625, -0.6019,\n",
       "          0.1647,  0.8892, -0.0255,  0.5188, -0.6625, -0.6019,  0.1647,  0.8892],\n",
       "        [ 1.1193, -0.9448,  0.5424, -0.4241, -0.0626, -1.5036,  1.1193, -0.9448,\n",
       "          0.5424, -0.4241, -0.0626, -1.5036,  1.1193, -0.9448,  0.5424, -0.4241,\n",
       "         -0.0626, -1.5036,  1.1193, -0.9448,  0.5424, -0.4241, -0.0626, -1.5036],\n",
       "        [ 0.2396, -0.5232, -0.5933, -0.1551, -3.1055, -0.5005,  0.2396, -0.5232,\n",
       "         -0.5933, -0.1551, -3.1055, -0.5005,  0.2396, -0.5232, -0.5933, -0.1551,\n",
       "         -3.1055, -0.5005,  0.2396, -0.5232, -0.5933, -0.1551, -3.1055, -0.5005]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.repeat(1, 4) # Shape: [2, 4], but creates a new, larger memory block.\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37562d0b",
   "metadata": {},
   "source": [
    "## Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75750a",
   "metadata": {},
   "source": [
    "### add() / +: Adds two tensors or a tensor and a scalar.\n",
    "\n",
    "### sub() / -: Subtracts one tensor from another.\n",
    "\n",
    "### mul() / *: Performs element-wise multiplication (Hadamard product).\n",
    "\n",
    "### div() / /: Performs element-wise division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46559ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 16, 18, 20],\n",
      "        [22, 24, 26, 28],\n",
      "        [30, 32, 34, 36]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3,4], [5,6,7,8], [9,10,11,12]])  # 3x4 tensor\n",
    "b = torch.tensor([[13,14,15,16], [17,18,19,20], [21,22,23,24]])  # 3x4 tensor\n",
    "c = a + b # Element-wise addition\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c03cdf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 16, 18, 20],\n",
      "        [22, 24, 26, 28],\n",
      "        [30, 32, 34, 36]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.add(a, b) # Element-wise addition\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe80cb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-12, -12, -12, -12],\n",
      "        [-12, -12, -12, -12],\n",
      "        [-12, -12, -12, -12]])\n"
     ]
    }
   ],
   "source": [
    "c = a - b # Element-wise subtraction\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e2b9cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12, 12],\n",
      "        [12, 12, 12, 12],\n",
      "        [12, 12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.sub(b, a) # Element-wise subtraction\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab339692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 8])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2])\n",
    "b = torch.tensor([3, 4])\n",
    "print(a * b) # tensor([3, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a396467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0769, 0.1429, 0.2000, 0.2500],\n",
      "        [0.2941, 0.3333, 0.3684, 0.4000],\n",
      "        [0.4286, 0.4545, 0.4783, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.div(a, b)) # tensor([0.3333, 0.5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47868a84",
   "metadata": {},
   "source": [
    "### matmul(): \n",
    "- The generic matrix multiplication function. It handles 1D, 2D, and high-dimensional tensors with broadcasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "03c07fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication result shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Matrix (2x3) @ Matrix (3x2) -> (2x2)\n",
    "mat1 = torch.randn(2, 3)\n",
    "mat2 = torch.randn(3, 2)\n",
    "res = torch.matmul(mat1, mat2) # Or mat1 @ mat2\n",
    "print(\"Matrix multiplication result shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fab493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58,  64],\n",
       "        [139, 154]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "mat2 = torch.tensor([[7, 8], [9, 10], [11, 12]])\n",
    "res = torch.matmul(mat1, mat2) # Shape: [2, 2]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ecc4969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 30,  24,  18],\n",
       "        [ 84,  69,  54],\n",
       "        [138, 114,  90],\n",
       "        [192, 159, 126]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) # Shape: [4, 3]\n",
    "mat2 = torch.tensor([[9, 8, 7], [6, 5, 4], [3, 2, 1]]) # Shape: [3, 3]\n",
    "mat3 = torch.matmul(mat1, mat2) # Shape: [4, 3]\n",
    "mat3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e318725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.8558,  1.7894, -2.6648, -3.5824,  5.1454],\n",
       "        [-2.0461, -1.6274,  0.8376,  1.0412, -1.2940],\n",
       "        [-3.8472, -0.6393,  0.0744,  3.2738, -0.8600],\n",
       "        [ 1.1481,  0.1155, -0.9582, -1.5022,  2.0582]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(4, 3)\n",
    "mat2 = torch.randn(3, 5)\n",
    "mat3 = torch.matmul(mat1, mat2) # Shape: [4, 5]\n",
    "mat3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a296ebf",
   "metadata": {},
   "source": [
    "### mm(): \n",
    "- A stricter version of matrix multiplication that only works for two 2D tensors (matrices). Use this for speed when you know your data is 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "46752958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9101, -0.2616],\n",
       "        [ 0.7204,  0.8979]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 3) # 2D tensor\n",
    "mat2 = torch.randn(3, 2) # 2D tensor\n",
    "res = torch.mm(mat1, mat2) # 2D Matrix multiplication\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8e865",
   "metadata": {},
   "source": [
    "### bmm(): \n",
    "- Batch Matrix Multiplication. Used when you have a batch of matrices (e.g., shape $[B, N, M]$) and want to multiply them without a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "79367c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.4323, -1.5249, -0.9572, -1.9377,  0.2226],\n",
       "         [-1.5481, -0.7953, -0.8928, -1.3014,  0.0399],\n",
       "         [-0.4902,  0.5933,  0.1729,  0.4578, -0.4212],\n",
       "         [-0.4300, -0.9472, -0.3385, -0.9112,  0.3932]],\n",
       "\n",
       "        [[-2.7766,  2.9747, -1.2182,  4.4545, -2.1316],\n",
       "         [-0.5441,  1.1734,  0.2189,  1.1327, -0.2883],\n",
       "         [-0.3703,  0.5993,  0.7820,  0.2181, -0.4616],\n",
       "         [ 0.2117, -0.0614, -0.4298,  0.1176,  0.3820]]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1 = torch.randn(2, 4, 3)\n",
    "mat2 = torch.randn(2, 3, 5)\n",
    "res = torch.bmm(mat1, mat2) # Shape: [2, 4, 5]\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5ff9f",
   "metadata": {},
   "source": [
    "### sum(), mean(), std(), var(): \n",
    "- Computes the sum, average, standard deviation, and variance. You can compute these across the whole tensor or along a specific dim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c4ecb5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0703,  0.6337, -0.9541, -0.9055,  1.9880],\n",
       "        [-0.9852, -1.6449, -0.5266, -0.1445, -1.8555],\n",
       "        [ 1.4578, -0.6934,  0.8816, -0.2342,  1.0401],\n",
       "        [ 0.0774,  1.9772, -0.8595,  0.5405,  0.8523]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2c2513f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.sum(): tensor(0.7154)\n",
      "torch.sum(a, dtype=torch.float32): tensor(0.7154)\n",
      "a.sum(dim=1): tensor([ 0.8324, -5.1568,  2.4518,  2.5879])\n",
      "res.sum(dim=(0,1)): tensor([-8.3798,  2.0119, -2.6627,  2.2303, -2.2650])\n",
      "mat3.mean(): tensor(0.0159)\n",
      "mat3.mean(dim=1, keepdim=True): tensor([[ 0.9087],\n",
      "        [-0.6177],\n",
      "        [-0.3997],\n",
      "        [ 0.1723]])\n",
      "z.std(): tensor(1.0034)\n",
      "z.std(unbiased=False): tensor(1.0027)\n",
      "z.var(): tensor(1.0067)\n",
      "z.var(unbiased=False): tensor(1.0054)\n",
      "weights.mean(dim=1): tensor([ 0.3623,  0.3211,  0.1957, -0.4624,  0.6392,  0.0647,  0.6338, -0.9345,\n",
      "         0.2162, -0.3652], grad_fn=<MeanBackward1>)\n",
      "torch.from_numpy(n).sum(): tensor(6)\n"
     ]
    }
   ],
   "source": [
    "# SUM\n",
    "print(\"a.sum():\", a.sum())                          # sum all elements of 2D tensor 'a'\n",
    "print(\"torch.sum(a, dtype=torch.float32):\", torch.sum(a, dtype=torch.float32))  # specify dtype\n",
    "print(\"a.sum(dim=1):\", a.sum(dim=1))                # sum along columns -> per-row sums\n",
    "print(\"res.sum(dim=(0,1)):\", res.sum(dim=(0, 1)))   # sum over first two dims of 3D tensor -> shape [5]\n",
    "\n",
    "# MEAN\n",
    "print(\"mat3.mean():\", mat3.mean())                  # global mean\n",
    "print(\"mat3.mean(dim=1, keepdim=True):\", mat3.mean(dim=1, keepdim=True))  # mean per-row, keepdim\n",
    "\n",
    "# STD / VAR\n",
    "print(\"z.std():\", z.std())                          # standard deviation (unbiased=True by default)\n",
    "print(\"z.std(unbiased=False):\", z.std(unbiased=False))  # population std\n",
    "print(\"z.var():\", z.var())                          # variance (unbiased=True default)\n",
    "print(\"z.var(unbiased=False):\", z.var(unbiased=False))  # population variance\n",
    "\n",
    "# Useful aggregated examples\n",
    "print(\"weights.mean(dim=1):\", weights.mean(dim=1))  # per-row mean for a 2D tensor with requires_grad\n",
    "print(\"torch.from_numpy(n).sum():\", torch.from_numpy(n).sum())  # convert numpy array 'n' and sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac9e72",
   "metadata": {},
   "source": [
    "### max() / min(): \n",
    "- Returns the maximum or minimum values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5af8c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum and Minimum tensor(1.9880) tensor(-1.8555)\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum and Minimum\", a.max(), a.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648a375",
   "metadata": {},
   "source": [
    "### argmax() / argmin(): \n",
    "- Returns the indices of the maximum or minimum values. This is how you determine the \"predicted class\" in classification.\n",
    "\n",
    "- Note on keepdim: When reducing, the dimension usually disappears. Setting keepdim=True maintains the original number of dimensions, which is helpful for broadcasting later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fb39934c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.tensor([0.1, 0.7, 0.2])\n",
    "prediction = torch.argmax(probs) # tensor(1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d079a9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      " tensor([[ 0.0703,  0.6337, -0.9541, -0.9055,  1.9880],\n",
      "        [-0.9852, -1.6449, -0.5266, -0.1445, -1.8555],\n",
      "        [ 1.4578, -0.6934,  0.8816, -0.2342,  1.0401],\n",
      "        [ 0.0774,  1.9772, -0.8595,  0.5405,  0.8523]])\n",
      "argmax along dim=1: tensor([4, 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# argmax() example with a 2D tensor\n",
    "print(\"a:\\n\", a)\n",
    "predictions = torch.argmax(a, dim=1)  # Get index of max value per row\n",
    "print(\"argmax along dim=1:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73136836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = torch.tensor([0.1, 0.7, 0.2])\n",
    "prediction = torch.argmin(probs) # tensor(0)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0accc704",
   "metadata": {},
   "source": [
    "### abs(): Computes the absolute value element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1ceab497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutes value element-wise: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[-1, -2, -3, -4, -5], [6, 7, 8, 9, 10]])\n",
    "print(\"Absolutes value element-wise:\", torch.abs(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2c6b0100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      " tensor([[ 1.2701, -1.1278,  0.4384],\n",
      "        [ 0.3268,  1.7379,  0.7190]])\n",
      "Absolutes value element-wise: tensor([[1.2701, 1.1278, 0.4384],\n",
      "        [0.3268, 1.7379, 0.7190]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "print(\"Original x:\\n\", x)\n",
    "print(\"Absolutes value element-wise:\", torch.abs(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f68338",
   "metadata": {},
   "source": [
    "### exp(): Computes the exponential $e^x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "47d2b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x:\n",
      " tensor([[-1.0309,  0.6312,  1.8066],\n",
      "        [ 0.6729, -0.4591,  1.7287]])\n",
      "Exponential value:  tensor([[0.3567, 1.8799, 6.0897],\n",
      "        [1.9599, 0.6319, 5.6331]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3) # Uninitialized tensor\n",
    "print(\"Original x:\\n\", x)\n",
    "print(\"Exponential value: \", torch.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d5d2f",
   "metadata": {},
   "source": [
    "### log(): Computes the natural logarithm $\\ln(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1be579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5247, -0.7139, -0.5295],\n",
      "        [-1.5583,  0.4529,  0.7406]])\n",
      "Natural logarithm:  tensor([[-0.6450,     nan,     nan],\n",
      "        [    nan, -0.7920, -0.3003]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,3)\n",
    "print(x)\n",
    "print(\"Natural logarithm: \", torch.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae359c",
   "metadata": {},
   "source": [
    "### pow(): Computes the power $x^y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ff409dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(torch.pow(x, 2)) # tensor([1, 4, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c46a12e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  8, 27])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "print(torch.pow(x, 3)) # tensor([1, 8, 27])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacccd44",
   "metadata": {},
   "source": [
    "### norm(): \n",
    "- Computes the vector or matrix norm.\n",
    "    - L2 Norm (Euclidean): $\\sqrt{\\sum |x_i|^2}$ — the default.\n",
    "    - L1 Norm (Manhattan): $\\sum |x_i|$ — often used for regularization to encourage sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "998b7e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([3.0, 4.0])\n",
    "print(torch.norm(v, p=2)) # tensor(5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26015d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([1.0, -1.0, 2.0, -2.0])\n",
    "print(torch.norm(v, p=1)) # tensor(6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "14ee2fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3894, -1.4843,  1.1277, -0.3767],\n",
      "        [ 0.6728,  0.2587, -1.5491, -1.2233],\n",
      "        [ 0.6190,  0.8564, -0.1544, -1.9031]])\n",
      "tensor(3.5981)\n"
     ]
    }
   ],
   "source": [
    "v = torch.randn(3, 4)\n",
    "print(v)\n",
    "print(torch.norm(v, p='fro')) # Frobenius norm for 2D tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc9648",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "- It is the engine that automatically calculates derivatives (gradients) for your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698c31bc",
   "metadata": {},
   "source": [
    "### backward(): \n",
    "- This is the most famous method in PyTorch. When you call loss.backward(), PyTorch travels backward through the Computational Graph, calculating the gradient of the loss with respect to every tensor that has requires_grad=True."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afec15",
   "metadata": {},
   "source": [
    "- Imagine we want to calculate the gradient of a simple function: $z = (x + 2)^2$.\n",
    "\n",
    "- In calculus, the derivative $\\frac{dz}{dx}$ should be $2(x + 2)$. If $x=3$, the gradient is $2(3+2) = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2071af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y's creator: <AddBackward0 object at 0x10a6e65f0>\n",
      "z's creator: <PowBackward0 object at 0x10a6e6530>\n",
      "Gradient at x (dz/dx): 12.0\n",
      "Gradient at y (dz/dy): 12.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. requires_grad: We tell PyTorch to track this tensor\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# 2. forward pass\n",
    "y = x + 2\n",
    "# 3. grad_fn: y will have a 'AddBackward' function because it was created by addition\n",
    "print(f\"y's creator: {y.grad_fn}\") \n",
    "\n",
    "z = y ** 2\n",
    "print(f\"z's creator: {z.grad_fn}\") # 'PowBackward'\n",
    "\n",
    "# 4. retain_grad(): By default, y's grad is deleted to save memory. \n",
    "# We call this if we want to inspect y.grad later.\n",
    "y.retain_grad()\n",
    "\n",
    "# 5. backward(): Calculate the gradients\n",
    "z.backward()\n",
    "\n",
    "# 6. grad: Access the results\n",
    "print(f\"Gradient at x (dz/dx): {x.grad}\") # Output: 12.0\n",
    "print(f\"Gradient at y (dz/dy): {y.grad}\") # Output: 12.0 (thanks to retain_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8600d768",
   "metadata": {},
   "source": [
    "### grad: \n",
    "- After calling .backward(), the calculated gradients are stored in this attribute of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9f1539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If x is a weight, x.grad tells us: \n",
    "# \"How much will the loss change if I move x slightly?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414eadd",
   "metadata": {},
   "source": [
    "### grad_fn: \n",
    "- Every tensor created by an operation (like y = x + 2) has a grad_fn. This is a reference to the function that created it, allowing PyTorch to trace the path back to the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2ef93e",
   "metadata": {},
   "source": [
    "### zero_grad(): \n",
    "- Gradients in PyTorch accumulate (add up) by default. Before you start a new calculation, you must call this to wipe the old gradients clean, otherwise your model will get \"confused\" by previous training steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de8605",
   "metadata": {},
   "source": [
    "### Efficiency & Inference \n",
    "- When you are just using a model to make predictions (Inference), you don't need gradients. Disabling them saves massive amounts of memory and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e649db6e",
   "metadata": {},
   "source": [
    "#### torch.no_grad(): \n",
    "- A context manager used during validation or testing. It tells PyTorch: \"Don't track anything here; I'm not training.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15a5d395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from model, use for inference\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # prediction = model(input_data)\n",
    "    print(\"Prediction from model, use for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bfb9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does detached x require grad? False\n",
      "Does cloned x require grad? True\n",
      "Is grad enabled? True\n",
      "Inside block, is grad enabled? False\n",
      "Forced back on: True\n"
     ]
    }
   ],
   "source": [
    "# 7. detach(): Creates a copy that is NOT part of the graph\n",
    "x_detached = x.detach()\n",
    "print(f\"Does detached x require grad? {x_detached.requires_grad}\") # False\n",
    "\n",
    "# 8. clone(): Creates a copy that IS still part of the graph\n",
    "x_cloned = x.clone()\n",
    "print(f\"Does cloned x require grad? {x_cloned.requires_grad}\") # True\n",
    "\n",
    "# 9. torch.no_grad(): Use this for evaluation/inference to save memory\n",
    "print(f\"Is grad enabled? {torch.is_grad_enabled()}\") # True\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 10. is_grad_enabled() and torch.enable_grad()\n",
    "    print(f\"Inside block, is grad enabled? {torch.is_grad_enabled()}\") # False\n",
    "    \n",
    "    with torch.enable_grad():\n",
    "        print(f\"Forced back on: {torch.is_grad_enabled()}\") # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff69d7",
   "metadata": {},
   "source": [
    "#### torch.enable_grad(): \n",
    "- The opposite of no_grad, used if you need to force-enable tracking within a block that was otherwise disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8e20e",
   "metadata": {},
   "source": [
    "#### is_grad_enabled(): \n",
    "- A simple check to see if PyTorch is currently recording history for gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a8e204",
   "metadata": {},
   "source": [
    "### Graph Surgery\n",
    "- Sometimes you need to \"break\" the graph or save specific parts of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606c1c0",
   "metadata": {},
   "source": [
    "#### detach(): \n",
    "- Returns a new tensor that is \"cut off\" from the computational graph. It shares the same data but will never track gradients. This is common when you want to use a value for a side-calculation without affecting the model's training.\n",
    "\n",
    "#### clone(): \n",
    "- Creates a copy of the tensor that stays in the graph. If you modify the clone, the gradients will still flow back to the original source.\n",
    "\n",
    "#### retain_grad(): \n",
    "- By default, PyTorch only saves gradients for \"leaf\" tensors (your weights). If you want to see the gradient for an intermediate activation (like the output of a hidden layer), you must call this on that tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer \n",
    "optimizer = torch.optim.SGD([x], lr=0.1)\n",
    "\n",
    "# In a real training loop:\n",
    "optimizer.zero_grad()   # 1. Clear old gradients from the previous step\n",
    "z = (x + 2)**2          # 2. Forward pass\n",
    "z.backward()            # 3. Backward pass\n",
    "optimizer.step()        # 4. Update x using the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ca69c",
   "metadata": {},
   "source": [
    "## Neural Network Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7371c3",
   "metadata": {},
   "source": [
    "### nn.Module: \n",
    "- The base class for all neural network modules. Your models should subclass this. \n",
    "- It automatically tracks all parameters defined inside it.\n",
    "\n",
    "### forward(): \n",
    "- The method must override. It defines the computation performed at every call. \n",
    "- Never call .forward() directly; just call model(input).\n",
    "\n",
    "### parameters(): \n",
    "- An iterator that yields every trainable weight and bias in the model. This is what you pass to an optimizer.\n",
    "\n",
    "### state_dict(): \n",
    "- A Python dictionary that maps each layer to its parameter tensor. This is the standard way to save or resume training.\n",
    "\n",
    "### load_state_dict(): \n",
    "- Used to load a saved state_dict into a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cc069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module): # Inherit from torch.nn.Module\n",
    "    def __init__(self): # Constructor\n",
    "        super(SimpleNN, self).__init__() # Call the parent constructor\n",
    "        self.fc1 = torch.nn.Linear(10, 5)  # Input layer to hidden layer\n",
    "        self.fc2 = torch.nn.Linear(5, 2)   # Hidden layer to output layer\n",
    "\n",
    "    def forward(self, x): # Define the forward pass, it never needs to be called directly\n",
    "        # Just call the model instance like a function: model(input)\n",
    "        x = torch.relu(self.fc1(x))  # Activation function\n",
    "        x = self.fc2(x) # No activation on output layer\n",
    "        return x # Return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9da6214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1224, -0.0819]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 10) # Example input tensor with 10 features\n",
    "model = SimpleNN()\n",
    "output = model(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03b7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: fc1.weight | Size: torch.Size([5, 10]) \n",
      "Values : tensor([[-0.2232,  0.3090,  0.0280,  0.1164, -0.1901, -0.0300,  0.0954,  0.1738,\n",
      "          0.0018, -0.2146],\n",
      "        [-0.2009, -0.1687,  0.0870,  0.2619, -0.0247, -0.2952, -0.0962, -0.2929,\n",
      "         -0.2517, -0.2027]], grad_fn=<SliceBackward0>)\n",
      "Layer: fc1.bias | Size: torch.Size([5]) \n",
      "Values : tensor([0.2136, 0.2215], grad_fn=<SliceBackward0>)\n",
      "Layer: fc2.weight | Size: torch.Size([2, 5]) \n",
      "Values : tensor([[ 0.3635, -0.4347,  0.2445,  0.2259, -0.2812],\n",
      "        [ 0.1511,  0.1182, -0.4360,  0.1981, -0.4298]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Layer: fc2.bias | Size: torch.Size([2]) \n",
      "Values : tensor([-0.2326, -0.1785], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} \\nValues : {param[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d71fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.2232,  0.3090,  0.0280,  0.1164, -0.1901, -0.0300,  0.0954,  0.1738,\n",
       "                        0.0018, -0.2146],\n",
       "                      [-0.2009, -0.1687,  0.0870,  0.2619, -0.0247, -0.2952, -0.0962, -0.2929,\n",
       "                       -0.2517, -0.2027],\n",
       "                      [-0.0733, -0.0770,  0.1848, -0.0075, -0.1479, -0.1372, -0.1625, -0.0243,\n",
       "                       -0.2798, -0.1647],\n",
       "                      [ 0.1529,  0.2301,  0.0197, -0.1608,  0.3020,  0.1200,  0.3121,  0.2732,\n",
       "                       -0.0358, -0.2938],\n",
       "                      [ 0.2966,  0.1813,  0.3068, -0.1949, -0.1947,  0.1374,  0.1867, -0.1720,\n",
       "                       -0.1029, -0.1559]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.2136,  0.2215, -0.3161, -0.2559, -0.1024])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.3635, -0.4347,  0.2445,  0.2259, -0.2812],\n",
       "                      [ 0.1511,  0.1182, -0.4360,  0.1981, -0.4298]])),\n",
       "             ('fc2.bias', tensor([-0.2326, -0.1785]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict() # Returns a dictionary of all model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a5999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state_dict\n",
    "torch.save(model.state_dict(), 'simple_nn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3d41cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3601, -0.0694]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model: create new instance, then load state_dict\n",
    "model_load = SimpleNN()  # Create a new instance of the model\n",
    "model_load.load_state_dict(torch.load('simple_nn.pth', weights_only=True))  # Load the saved parameters\n",
    "model_load.eval()  # Set to evaluation mode (disables dropout, batch norm, etc.)\n",
    "\n",
    "# Now we can use it for inference\n",
    "x = torch.randn(1, 10)  # Example input tensor with 10 features\n",
    "with torch.no_grad():   # Disable gradient tracking during inference\n",
    "    output = model_load(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445a8bb",
   "metadata": {},
   "source": [
    "### nn.Linear: \n",
    "- A fully connected (dense) layer. It performs $y = xA^T + b$.\n",
    "\n",
    "### nn.Conv2d: \n",
    "- It working with Computer Vision. It slides a kernel over an image to extract features like edges or textures.\n",
    "\n",
    "### nn.ConvTranspose2d: \n",
    "- Often called \"deconvolution.\" It increases the spatial resolution of a tensor (upsampling), commonly used in Image Generation (GANs) or Segmentation (U-Net)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24af4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c968d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear out shape: torch.Size([1, 5])\n",
      "tensor([[ 0.3747,  0.6029,  0.5108, -0.7281, -0.0622]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "Conv2d out shape: torch.Size([1, 6, 64, 64])\n",
      "ConvTranspose2d out shape: torch.Size([1, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Linear example (uses existing x: [1,10])\n",
    "linear = torch.nn.Linear(10, 5)\n",
    "linear_out = linear(x)  # x defined in earlier cell\n",
    "print(\"Linear out shape:\", linear_out.shape)\n",
    "print(linear_out)\n",
    "\n",
    "# Conv2d example\n",
    "conv = torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n",
    "inp_conv = torch.randn(1, 3, 64, 64)\n",
    "conv_out = conv(inp_conv)\n",
    "print(\"Conv2d out shape:\", conv_out.shape)\n",
    "\n",
    "# ConvTranspose2d example\n",
    "convT = torch.nn.ConvTranspose2d(in_channels=6, out_channels=3, kernel_size=4, stride=2, padding=1)\n",
    "convT_out = convT(conv_out)\n",
    "print(\"ConvTranspose2d out shape:\", convT_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a9b3a0",
   "metadata": {},
   "source": [
    "### nn.MaxPool2d: \n",
    "- Reduces the spatial size by taking the maximum value in a window. It provides \"translation invariance\" (the model recognizes a feature even if it's shifted).\n",
    "\n",
    "### nn.AvgPool2d: \n",
    "- Similar to MaxPool, but takes the average.\n",
    "\n",
    "### nn.BatchNorm2d: \n",
    "- Normalizes the activations of a layer. It makes training much faster and less sensitive to initial weight values.\n",
    "\n",
    "### nn.LayerNorm: \n",
    "- Normalizes across the features of a single sample (standard in Transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab00624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxPool2d out shape: torch.Size([1, 6, 32, 32])\n",
      "AvgPool2d out shape: torch.Size([1, 6, 32, 32])\n",
      "BatchNorm2d out shape: torch.Size([1, 6, 64, 64])\n",
      "LayerNorm out shape: torch.Size([1, 6, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# MaxPool2d example\n",
    "maxpool = torch.nn.MaxPool2d(kernel_size=2, stride=2) # Downsamples by a factor of 2\n",
    "maxpool_out = maxpool(conv_out) \n",
    "print(\"MaxPool2d out shape:\", maxpool_out.shape) # Should be [1, 6, 32, 32]\n",
    "\n",
    "# AvgPool2d example\n",
    "avgpool = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "avgpool_out = avgpool(conv_out)\n",
    "print(\"AvgPool2d out shape:\", avgpool_out.shape)\n",
    "\n",
    "# BatchNorm2d example\n",
    "batchnorm = torch.nn.BatchNorm2d(num_features=6)\n",
    "batchnorm_out = batchnorm(conv_out)\n",
    "print(\"BatchNorm2d out shape:\", batchnorm_out.shape)\n",
    "\n",
    "# LayerNorm example\n",
    "layernorm = torch.nn.LayerNorm(normalized_shape=[64, 64])\n",
    "layernorm_out = layernorm(conv_out)\n",
    "print(\"LayerNorm out shape:\", layernorm_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e616e",
   "metadata": {},
   "source": [
    "### nn.Dropout: \n",
    "- Randomly \"turns off\" neurons during training. This forces the network to not rely too heavily on any single neuron, preventing overfitting.\n",
    "\n",
    "### nn.Sequential: \n",
    "- A quick way to wrap layers if data flows through them in a strict linear order.\n",
    "\n",
    "### nn.ModuleList: \n",
    "- Like a Python list for layers. Use this if you need to iterate through layers in a loop (PyTorch won't \"see\" layers inside a standard Python list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9016d390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mode - some values zeroed out:\n",
      "tensor([[-0.0000,  0.0000,  0.1685, -0.0000,  0.0000,  1.0846,  0.0000,  1.3448,\n",
      "         -0.0000,  2.0884],\n",
      "        [-2.9705, -0.0810, -0.5904, -0.0000, -0.0000,  0.8355, -0.0000,  1.7615,\n",
      "         -0.0000,  1.6374],\n",
      "        [ 1.0517, -4.2785,  0.0000,  3.8123,  0.9118,  1.9260,  0.8287, -1.7343,\n",
      "         -2.2264,  0.0000],\n",
      "        [-0.0363,  0.0000, -1.1881,  3.5526,  0.0000, -0.0000,  2.6830,  0.0000,\n",
      "          4.3482, -1.3131]])\n",
      "\n",
      "Inference mode - no dropout:\n",
      "tensor([[-0.3213,  0.2287,  0.0842, -0.9930,  1.7439,  0.5423,  0.2202,  0.6724,\n",
      "         -1.3083,  1.0442],\n",
      "        [-1.4852, -0.0405, -0.2952, -0.8329, -0.3797,  0.4178, -0.1244,  0.8807,\n",
      "         -0.8680,  0.8187],\n",
      "        [ 0.5259, -2.1392,  0.2009,  1.9061,  0.4559,  0.9630,  0.4144, -0.8672,\n",
      "         -1.1132,  0.3636],\n",
      "        [-0.0182,  1.5780, -0.5940,  1.7763,  0.1072, -1.2694,  1.3415,  0.5940,\n",
      "          2.1741, -0.6565]])\n"
     ]
    }
   ],
   "source": [
    "# Dropout example: Randomly disables 50% of neurons during training\n",
    "dropout = torch.nn.Dropout(p=0.5)  # p is the probability of dropping a neuron\n",
    "\n",
    "# During training (model.train() is default)\n",
    "training_input = torch.randn(4, 10)\n",
    "training_output = dropout(training_input)\n",
    "print(f\"Training mode - some values zeroed out:\")\n",
    "print(training_output)\n",
    "\n",
    "# During inference (model.eval())\n",
    "dropout.eval()\n",
    "with torch.no_grad():\n",
    "    inference_output = dropout(training_input)  # No dropout applied\n",
    "    print(f\"\\nInference mode - no dropout:\")\n",
    "    print(inference_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a89908e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential model output shape: torch.Size([2, 5])\n",
      "tensor([[-0.1674,  0.3989,  0.1557,  0.0162, -0.0391],\n",
      "        [-0.0863, -0.1511,  0.3430,  0.0261, -0.0790]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "First layer: Linear(in_features=10, out_features=64, bias=True)\n",
      "Second layer: ReLU()\n"
     ]
    }
   ],
   "source": [
    "# Sequential example: Build a simple feedforward network in linear order\n",
    "model_sequential = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(64, 32),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(32, 5)  # Output layer (5 classes)\n",
    ")\n",
    "\n",
    "# Test the model\n",
    "test_input = torch.randn(2, 10)  # Batch of 2 samples, 10 features each\n",
    "output = model_sequential(test_input)\n",
    "print(f\"Sequential model output shape: {output.shape}\")  # [2, 5]\n",
    "print(output)\n",
    "\n",
    "# Access individual layers\n",
    "print(f\"\\nFirst layer: {model_sequential[0]}\")\n",
    "print(f\"Second layer: {model_sequential[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43e3f00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicNet output shape: torch.Size([3, 2])\n",
      "tensor([[-0.1535, -0.0767],\n",
      "        [-0.1385, -0.0092],\n",
      "        [-0.1892,  0.0138]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Total layers in ModuleList: 8\n",
      "Layer 0: Linear(in_features=10, out_features=32, bias=True)\n",
      "Layer 1: Linear(in_features=32, out_features=32, bias=True)\n",
      "Layer 2: ReLU()\n",
      "Layer 3: Dropout(p=0.3, inplace=False)\n",
      "Layer 4: Linear(in_features=32, out_features=32, bias=True)\n",
      "Layer 5: ReLU()\n",
      "Layer 6: Dropout(p=0.3, inplace=False)\n",
      "Layer 7: Linear(in_features=32, out_features=2, bias=True)\n",
      "\n",
      "Total trainable parameters: 2530\n"
     ]
    }
   ],
   "source": [
    "# ModuleList example: Use when you need dynamic or iterative layer access\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, num_layers=3, input_size=10, hidden_size=32, output_size=2):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        \n",
    "        # ModuleList: PyTorch will track these layers (standard Python list won't!)\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        \n",
    "        # First layer\n",
    "        self.layers.append(torch.nn.Linear(input_size, hidden_size))\n",
    "        \n",
    "        # Hidden layers (dynamic)\n",
    "        for i in range(num_layers - 2):\n",
    "            self.layers.append(torch.nn.Linear(hidden_size, hidden_size))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Dropout(p=0.3))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(torch.nn.Linear(hidden_size, output_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Iterate through all layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Create and test the model\n",
    "model_modulelist = DynamicNet(num_layers=4, input_size=10, hidden_size=32, output_size=2)\n",
    "test_input = torch.randn(3, 10)  # Batch of 3 samples\n",
    "output = model_modulelist(test_input)\n",
    "print(f\"DynamicNet output shape: {output.shape}\")  # [3, 2]\n",
    "print(output)\n",
    "\n",
    "# Print all layers to verify PyTorch tracked them\n",
    "print(f\"\\nTotal layers in ModuleList: {len(model_modulelist.layers)}\")\n",
    "for i, layer in enumerate(model_modulelist.layers):\n",
    "    print(f\"Layer {i}: {layer}\")\n",
    "\n",
    "# Show that ModuleList parameters are tracked by the model\n",
    "total_params = sum(p.numel() for p in model_modulelist.parameters())\n",
    "print(f\"\\nTotal trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352977ed",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
